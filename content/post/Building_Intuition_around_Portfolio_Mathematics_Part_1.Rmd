---
title: "Building Intuition around Portfolio Mathematics"
subtitle: "Part 1"
author: "John W. Pearce, CFA"
date: "2019-05-22"
tags: ["R Markdown", "Portfolio Mathematics", "Quant", "tidyquant", "tidyverse", "PerformanceAnalytics"]
# output:
#   html_document:
#     number_sections: yes
#     toc: yes
#     toc_float: true
#     toc_depth: 2
#     theme: spacelab
#     # fig_width: 10
#     # fig_height: 6
---



($\alpha$)typicalquant

# Introduction and Motivation

If you've found your way to this blog post you likely already understand the power of `R` for solving all manner of financial mathematics problems.  Packages like `PerformanceAnalytics` make quick work of many of the most frequently required calculations and the `tidyquant` wrapper helps keep results nice and tidy. Unfortunately, calculations describing relationships between assets often return less readily consumable output. 

This work stems from investigation of higher order moments. My colleague Bryan Franco and I have been interested in learning more about the importance of the coskewness and cokurtosis structures of assets as it relates to pricing and portfolio construction. It didn't take long for us to reach a point where we needed a better tool to help us understand the output of the calculations we were making. 

In order to demonstrate the issue, I'll pull in the `edhec` dataset.

```{r setup, warning=FALSE, message=FALSE}
# Load library that includes required data and functions
library(PerformanceAnalytics)

# Attach required data to the environment
data(edhec)
```

When investingating covariance or correlation the two dimensional output is well suited to display and can be evaluated using basic functions. We can use the function `cov()` from the `stats` package to return a covariance matrix that is well labeled and, therefore, understandable.

```{r cov_edhec_1_2}
cov(edhec[, 1:2])
```

As one moves into investigating higher order moments, however, things get more complicated. Using the `CoSkewnessMatrix()` function from `PerformanceAnalytics` on just the first two columns of `edehc` returns the following output:

```{r csk_edhec_1_2}
CoSkewnessMatrix(edhec[, 1:2])
```

As expected, we return a matrix of dimension $n \times n^{2}$. But now we have a bit of a dilemma --- how can we reliably select the element $(i, j, j)$ from the coskewness matrix without accidentally selecting the element $(i, i, j)$? Moreover, which elements represent the diagonal? All of these complications stem from the object being stored as a two dimensional representation of what is inherently a three dimensional object, a cube.

This problem gets worse as the order grows. Consider the fourth moment where we return a matrix of $n \times n^{3}$. How does one select the element $(i, j, j, j)$ of the cokurtosis matrix? Now our ability to grasp what we are doing is even lower as we are attempting to select from a four dimensional hypercube.

```{r}
CoKurtosisMatrix(edhec[, 1:2])
```

And when we expand the number of assets included in the matrix we make a complicated idea even more complex.

```{r, eval=FALSE, echo=FALSE}
CoKurtosisMatrix(edhec[, 1:4])
```

Functions that report the "systematic comoment" such as `CoSkewness()` and `CoKurtosis()` take arguments Ra and Rb and report the elements $(Ra, Rb, Rb)$ and $(Ra, Rb, Rb, Rb)$ respectively so there are ways to pick off specific elements of the matrix [^1]. Unfortunately, these approahces do not shed light on how to visualize the entirety of the matrix at once which is essential, as I will demonstrate in a future post, if we are to need to calculate the resulting variance, skewness, or kurtosis of a portfolio of assets with a known weight vector. Further, all of these approaches suffer from abstraction away from what is actually included in the calculation of a given element of a comoment matrix.

[^1]: However, I am not currently aware of a way to return the cokurtosis for element $(Ra, Ra, Rb, Rb)$ which may be useful for some applications.

The usual methods of answering these questions are more computationally efficient than the approach I propose below. But for those of us who are not blessed with the ability to natively visualize and understand the transforms performed with matrix multiplication or tensor products this approach offers great benefits. I save this treatment for another post, Part 2.

Further, I think that teaching people to reshape data sets and perform subsequent mutations or summaries using the relatively easy to grasp vernacular of the `tidyverse` can help to demystify all manner of problems for those who were either never taught or never fully grasped concepts from linear algebra and vectorized computing. I'll go out on a little bit of a limb here to say that perhaps an approach like this would be useful for helping people to learn those sorts of concepts in much the same manner that common core mathematics, while on the surface "computationally inefficient" and derided by many as "fuzzy", is helping kids increase numeracy by making ideas more intuitive.

# Setup and Data Preprocessing

We already loaded in the `edhec` dataset and functions from `PerformanceAnalytics` above. This leaves us with one more library to load, the `tidyverse` package. 

```{r setup2, warning=FALSE, message=FALSE}

# Load the required library.
library(tidyverse)

```

Now that we have all of the required tools in place, we need to prepare the `edhec` data for use in a tidy workflow.

The following step returns a "long" version of `edhec` where we have columns for Date, Asset, and Return. This is the data structure we will use to make our tidy comoment calculations.

```{r edhec_tidy, warning=FALSE, message=FALSE}

edhec_tidy <- edhec %>% 
  as_tibble() %>% # Start by making a tibble from the xts object
  mutate(Date = index(edhec)) %>% # Create a Date column from the xts index
  gather(Asset, Return, -Date) # gather to make the wide edhec data long
edhec_tidy
```

# Explaination of Approach

So what is the general process for making the required calculations? We need to consider for all combinations of assets for every day over a date range the product of each assets deviation from its mean with the same measure for the others. We can use some data reshapign concepts to achieve this and retain identifying information about which asset is which.

To do so, we "trick" the data into the "lengthwise-correct" dimensionality by using a data widening step, `spread()`. This process produces columns that contain missing values which can be used to our advantage. After widening, we immediately re-lengthen using `gather()` respectively and the missing values extend the length of the table to the "lengthwise-correct" dimensionality. All that remains from here is to fill in those missing values. We repeat this process until we have achieved the required dimensionality for covariance, coskewness, or cokurtosis.

# Creation of Functions

Understanding the basic principle, I have outlined as functions the process for covariance, coskewness, and cokurtosis.

## Covariance

Although it is clear that using the `cov()` function is sufficient to produce an understandable output object, the process builds on itself such that it makes sense to start with covariance and build from there. Further, having the covariance matrix represented in tidy format will be useful when we go on to calculate portfolio variance from it in a future post.[^2]

[^2]: You could use `gather()` on the output of `cov()` after first adding a column for row name using `rownames_to_column()`. This approach may be faster, which I will investigate in another post, but still loses the advantage of transparency and teachability.

The formula for the covariance of any two random variables $i$ and $j$ is given to be:

$$CoV(i, j) = E[(i - \overline{i})(j - \overline{j})]$$

Said in English, for each pairwise-complete observation (in this case, every date for an asset pair), we need to multiply the deviation of return for each asset $i$ from its mean by the deviation of each asset $j$ from its mean. We then take the expected value (average) of those resulting products to arrive at the covariance of each asset pair.

To perform these steps using the approach described above, begin by selecting the required columns from the data and grouping by asset.

```{r}
edhec_tidy_1 <- edhec_tidy %>% 
    select(Date, Asset, Return) %>% 
    group_by(Asset)
edhec_tidy_1
```

Then mutate to create a column for the difference between the point value of return and the mean value of return for each aset. Drop the Return column as we no longer require it.

```{r}
edhec_tidy_2 <- edhec_tidy_1 %>% 
  mutate(Point_Deviation = Return - mean(Return)) %>% 
  select(-Return)
edhec_tidy_2
```

Now is when the magic starts to happen. Create a copy of the Asset column and save it as Asset.i. This identifies which Asset is the $i$ element. Then we perform the first data reshaping step to widen the data. 

```{r}
edhec_tidy_3 <- edhec_tidy_2 %>% 
  mutate(Asset.i = Asset
         , Point_Deviation.i = Point_Deviation
         ) %>% 
  spread(Asset, Point_Deviation) 
edhec_tidy_3
```

Doing this created a lot of missing values which we will use to create a longer tibble that has rows for each combination of date and pair of assets $i$ and $j$. This is done using `gather()`.

```{r}
edhec_tidy_4 <- edhec_tidy_3 %>% 
  gather(Asset.j, Point_Deviation.j, -Date
         , -Asset.i, -Point_Deviation.i
         ) 
edhec_tidy_4
```

The result is a longer tibble but where many of the values are missing. Those need to be filled in with the only relevant data point for each combination of Date and Asset.j. I chose to use `max()` to do this.

```{r}
edhec_tidy_5 <- edhec_tidy_4 %>% 
  group_by(Date, Asset.j) %>% 
    mutate(Point_Deviation.j = max(Point_Deviation.j, na.rm = TRUE)
           , Joint_Point_Deviation = Point_Deviation.i * Point_Deviation.j
           ) 
edhec_tidy_5
```

Finally, we group by Asset.i and Asset.j such that we return a summary value for each pair of $i$ and $j$ which is the mean (in this case adjusted for $n-1$ observations) of the "joint point deviation".

```{r}
edhec_covar_tidy <- edhec_tidy_5 %>% 
    group_by(Asset.i, Asset.j) %>% 
    summarise(CoVar = sum(Joint_Point_Deviation) / (length(Joint_Point_Deviation) - 1)) %>% 
    ungroup()
edhec_covar_tidy
```

A comparison of these values and those resulting from using the `cov()` function can be found later in this post but for now take my word for it that they are correct.

Putting all of the above steps together, the following function applies this logic and returns a tibble of covariance between all combinations of pairs of assets:

```{r}

covariance_tbl <- function (input_tibble) {
  
  if (("tbl" %in% class(input_tibble)) != TRUE) stop ("The input data is not in tibble format.")
  if (all(c("Date", "Asset", "Return") %in% colnames(input_tibble)) != TRUE) stop ("The input data requires fields 'Date', 'Asset', and 'Return'.")
  
  # Calculate CoVariance Tibble
  output_tibble <- input_tibble %>% 
    select(Date, Asset, Return) %>% 
    group_by(Asset) %>% 
    mutate(Point_Deviation = Return - mean(Return)) %>% 
    select(-Return) %>% 
    mutate(Asset.i = Asset
           , Point_Deviation.i = Point_Deviation
           ) %>% 
    spread(Asset, Point_Deviation) %>% 
    gather(Asset.j, Point_Deviation.j, -Date
           , -Asset.i, -Point_Deviation.i
           ) %>% 
    group_by(Date, Asset.j) %>% 
    mutate(Point_Deviation.j = max(Point_Deviation.j, na.rm = TRUE)
           , Joint_Point_Deviation = Point_Deviation.i * Point_Deviation.j
           ) %>% 
    group_by(Asset.i, Asset.j) %>% 
    summarise(CoVar = sum(Joint_Point_Deviation) / (length(Joint_Point_Deviation) - 1)) %>% 
    ungroup()
  
  return(output_tibble)
  
}

```


## Coskewness

<!-- 
$$S(i, j, k) = \frac{E[(i - \overline{i})(j - \overline{j})(k - \overline{k})]}{\sigma_{i} \sigma_{j} \sigma_{k}}$$


However, much like in the case of coskewness, the `CoKurtosisMatrix()` function returns the unadjusted mean of the individual asset joint point deviance.
-->

$$CoS(i, j, k) = E[(i - \overline{i})(j - \overline{j})(k - \overline{k})]$$

Calculating coskewness using the same method for covariance requires just one additional itearation of the procedure of adding a duplication of asset i's name and point deviation from its mean, widening and lengthening the data to increase the dimensionality, then filling in missing values and summarising. I have not recreated the step by step explaination for coskewness as it is simply a second iteration of that process.

```{r}

coskewness_tbl <- function (input_tibble) {
  
  if (("tbl" %in% class(input_tibble)) != TRUE) stop ("The input data is not in tibble format.")
  if (all(c("Date", "Asset", "Return") %in% colnames(input_tibble)) != TRUE) stop ("The input data requires fields 'Date', 'Asset', and 'Return'.")
  
  # Calculate CoSkewness Tibble
  output_tibble <- input_tibble %>% 
    select(Date, Asset, Return) %>% 
    group_by(Asset) %>% 
    mutate(Point_Deviation = Return - mean(Return)) %>% 
    select(-Return) %>% 
    mutate(Asset.i = Asset
           , Point_Deviation.i = Point_Deviation
           ) %>% 
    spread(Asset, Point_Deviation) %>% 
    gather(Asset.j, Point_Deviation.j, -Date
           , -Asset.i, -Point_Deviation.i
           ) %>% 
    group_by(Date, Asset.j) %>% 
    mutate(Point_Deviation.j = max(Point_Deviation.j, na.rm = TRUE)
           , Asset.k = Asset.i
           , Point_Deviation.k = Point_Deviation.i
           ) %>% 
    spread(Asset.k, Point_Deviation.k) %>% 
    gather(Asset.k, Point_Deviation.k, -Date
           , -Asset.i, -Point_Deviation.i
           , -Asset.j, -Point_Deviation.j
           ) %>% 
    group_by(Date, Asset.k) %>% 
    mutate(Point_Deviation.k = max(Point_Deviation.k, na.rm = TRUE)
           , Joint_Point_Deviation = Point_Deviation.i * Point_Deviation.j * Point_Deviation.k
           ) %>% 
    group_by(Asset.i, Asset.j, Asset.k) %>% 
    summarise(CoSkew = mean(Joint_Point_Deviation)) %>% 
    ungroup()
  
  return(output_tibble)
  
}

```

## Cokurtosis

For formula for the skewness of any four random variables is given to be:
<!--
$$K(i, j, k, l) = \frac{E[(i - \overline{i})(j - \overline{j})(k - \overline{k})(l - \overline{l})]}{\sigma_{i} \sigma_{j} \sigma_{k} \sigma_{l}}$$

However, much like in the case of coskewness, the `CoKurtosisMatrix()` function returns the unadjusted mean of the individual asset joint point deviance.
-->

$$CoK(i, j, k, l) = E[(i - \overline{i})(j - \overline{j})(k - \overline{k})(l - \overline{l})]$$

Functionally, much like going from covariance to coskewness, moving to cokurtosis requires just one additional itearation of the procedure of adding a duplication of asset i's name and point deviation from its mean, widening and lengthening the data to increase the dimensionality, then filling in missing values and summarising. Again, I've not included the play by play.

```{r}

cokurtosis_tbl <- function (input_tibble) {
  
  if (("tbl" %in% class(input_tibble)) != TRUE) stop ("The input data is not in tibble format.")
  if (all(c("Date", "Asset", "Return") %in% colnames(input_tibble)) != TRUE) stop ("The input data requires fields 'Date', 'Asset', and 'Return'.")
  
  # Calculate CoKurtosis Tibble
  output_tibble <- input_tibble %>% 
    select(Date, Asset, Return) %>% 
    group_by(Asset) %>% 
    mutate(Point_Deviation = Return - mean(Return)) %>% 
    select(-Return) %>% 
    mutate(Asset.i = Asset
           , Point_Deviation.i = Point_Deviation
           ) %>% 
    spread(Asset, Point_Deviation) %>% 
    gather(Asset.j, Point_Deviation.j, -Date, -Asset.i, -Point_Deviation.i) %>% 
    group_by(Date, Asset.j) %>% 
    mutate(Point_Deviation.j = max(Point_Deviation.j, na.rm = TRUE)
           , Asset.k = Asset.i
           , Point_Deviation.k = Point_Deviation.i
           ) %>% 
    spread(Asset.k, Point_Deviation.k) %>% 
    gather(Asset.k, Point_Deviation.k, -Date
           , -Asset.i, -Point_Deviation.i
           , -Asset.j, -Point_Deviation.j
           ) %>% 
    group_by(Date, Asset.k) %>% 
    mutate(Point_Deviation.k = max(Point_Deviation.k, na.rm = TRUE)
           , Asset.l = Asset.i
           , Point_Deviation.l = Point_Deviation.i
           ) %>% 
    spread(Asset.l, Point_Deviation.l) %>% 
    gather(Asset.l, Point_Deviation.l, -Date
           , -Asset.i, -Point_Deviation.i
           , -Asset.j, -Point_Deviation.j
           , -Asset.k, -Point_Deviation.k
           )   %>% 
    group_by(Date, Asset.l) %>% 
    mutate(Point_Deviation.l = max(Point_Deviation.l, na.rm = TRUE)
           , Joint_Point_Deviation = Point_Deviation.i * Point_Deviation.j * Point_Deviation.k * Point_Deviation.l
           ) %>% 
    group_by(Asset.i, Asset.j, Asset.k, Asset.l) %>% 
    summarise(CoKurt = mean(Joint_Point_Deviation)) %>% 
    ungroup()
  
  return(output_tibble)
  
}

```

## Even Higher Order Moments?

Absolutely. Simply continue the pattern of copying the name and Point Deviation for element i to a new colum, adding a data widening and lengthening step, filling in the missing values, and adjusting the "Joint_Point_Deviation" to incorporate the new asset.

# Comparison of Results

## Covariance

```{r}
edhec_covar_tidy <- edhec_tidy %>% 
  covariance_tbl()
edhec_covar_tidy
```

```{r}
cov(edhec[, 1:5])
```

```{r}
isTRUE(all.equal(as.numeric(edhec_covar_tidy$CoVar), as.numeric(cov(edhec)), tolerance = 0.0001))

as.numeric(edhec_covar_tidy$CoVar) %in% as.numeric(cov(edhec))


edhec_covar_tidy$CoVar[order(as.numeric(edhec_covar_tidy$CoVar))] - as.numeric(cov(edhec))[order(as.numeric(cov(edhec)))]

```

## Coskewness

```{r}
edhec_csk_tidy <- edhec_tidy %>% 
  coskewness_tbl()
edhec_csk_tidy
```

```{r}
CoSkewnessMatrix(edhec)[1:5, 1:5]
```

```{r}
isTRUE(all.equal(as.numeric(edhec_csk_tidy$CoSkew), as.numeric(CoSkewnessMatrix(edhec)), tolerance = 0.00001))
```


## Cokurtosis

# Looking Ahead

Having demonstrated the calculation procedure that generates a comoment table which is equivalent to a comoment matrix that can be achieved with functions in `PerformanceAnalytics` it is natural to ask what benefit the resulting table provides other better understanding in calculation along the way and human readability. 
